AWSTemplateFormatVersion: 2010-09-09

Parameters:
  BEARERTOKEN:
    Description: WebServer EC2 instance type
    Type: AWS::SSM::Parameter::Value<String>
    Default: BEARERTOKEN                                                                                                       

  Environmentval:
    Type: String
    Default: dev
    AllowedValues:
      - "dev"
      - "prod"

  DataBucket: 
    Type: String

  MetaBucket:
    Type: String

  PathIngestCrawler:
    Type: String

  GlueJobTempPath:
    Type: String

  PathProcessedTweets:
    Type: String

  PathGlueJobScript:
    Type: String

Mappings: 
  DatabaseNameMap:
    prod: 
      database: prod-tweets-db
    dev:
      database: dev-tweets-db

Resources: 
  LambdaExecutionRole:
    Properties:
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonSSMReadOnlyAccess
      AssumeRolePolicyDocument:
        Statement:
        - Action:
          - sts:AssumeRole
          Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
        Version: '2012-10-17'
      Path: "/"
      Policies:
      - PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Action: s3:*
            Effect: Allow
            Resource: "*"
        PolicyName: root
    Type: AWS::IAM::Role
  LambdaFunction:
    Properties:
      Code:
        S3Bucket: !Ref MetaBucket
        S3Key: lambda-source/TweetLoaderZIP.zip
      Handler: lambda_function.lambda_handler
      MemorySize: 128
      Role:
        Fn::GetAtt:
        - LambdaExecutionRole
        - Arn
      Runtime: python3.7
      Timeout: 180
      Environment: 
        Variables:
          BEARERTOKEN: 
            Ref: BEARERTOKEN
          ENVIRONMENTVAL:
            Ref: Environmentval
    Type: AWS::Lambda::Function
  LambdaFunctionSchedule:
    Properties:
      ScheduleExpression: cron(10 0/1 * * ? *)
      Targets:
      - Arn:
          Fn::GetAtt:
          - LambdaFunction
          - Arn
        Id: '1'
    Type: AWS::Events::Rule
  LambdaFunctionCwPermission:
    Properties:
      Action: lambda:InvokeFunction
      FunctionName:
        Fn::GetAtt:
        - LambdaFunction
        - Arn
      Principal: events.amazonaws.com
      SourceArn:
        Fn::GetAtt:
        - LambdaFunctionSchedule
        - Arn
    Type: AWS::Lambda::Permission

  ProcessTweetsWorkflow:
    Type: AWS::Glue::Workflow
    Properties: 
      Description: The workflows triggers the initial glue crawler for the ingested tweets. After competion a glue job is started to process the tweets. The processed tweets are processed in a glue crawler
      Name: !Join ["-", [!Ref Environmentval , "ProcessTweetsWorkflow"]]  # ProcessTweetsWorkflow

  RoleProcessingTweets:
    Type: AWS::IAM::Role
    Properties:
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonS3FullAccess
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
        - arn:aws:iam::aws:policy/AmazonSQSFullAccess
        - arn:aws:iam::aws:policy/PowerUserAccess
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          -
            Effect: "Allow"
            Principal:
              Service:
                - "glue.amazonaws.com"
            Action:
              - "sts:AssumeRole"
      Path: "/"
      Policies:
        -
          PolicyName: "root"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              -
                Effect: "Allow"
                Action: "*"
                Resource: "*"
 # Create a database to contain tables created by the crawler
 # Glue Database is created during the AWS configuration step in GitHub Actions since the retaining policy of the database caused sever problems and resource couldnt be imported as existing resource in the CloudFormation stack (not available here: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resource-import-supported-resources.html)

  #DatabaseIngestedTweets:
  #  Type: AWS::Glue::Database
  #  DeletionPolicy: Retain
  #  UpdateReplacePolicy: Retain
  #  Properties:
  #    CatalogId: !Ref AWS::AccountId
  #    DatabaseInput:
  #      Name: !FindInMap [DatabaseNameMap, !Ref Environmentval, database]
  #      Description: "Glue Databases for ingested tweets"
 

 #Create a crawler to crawl the tweets data in a s3 bucket
  CrawlerIngestedTweets:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ["-", [!Ref Environmentval , "CrawlerIngestedTweets"]] #old CrawlerIngestedTweets
      Role: !GetAtt RoleProcessingTweets.Arn
      # Just Crawl New Order to limit workload and costs
      RecrawlPolicy: 
        RecrawlBehavior: "CRAWL_NEW_FOLDERS_ONLY"
      #Classifiers: none, use the default classifier
      Description: AWS Glue crawler to crawl ingested tweets
      #Schedule: none, use default run-on-demand
      DatabaseName: !FindInMap [DatabaseNameMap, !Ref Environmentval, database]
      TablePrefix: !Join ["_", [!Ref Environmentval , ""]]
      Targets:
        S3Targets:
          # Private S3 bucket with ingested tweet data
          - Path: !Ref PathIngestCrawler
      SchemaChangePolicy:
        UpdateBehavior: "LOG" # Updated to LOG since RecrawPolicy required it --> "UPDATE_IN_DATABASE"
        DeleteBehavior: "LOG"
      #Configuration: "{\"Version\":1.0,\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"},\"Tables\":{\"AddOrUpdateBehavior\":\"MergeNewColumns\"}}}"
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableGroupingPolicy\":\"CombineCompatibleSchemas\"},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"},\"Tables\":{\"AddOrUpdateBehavior\":\"MergeNewColumns\"}}}"

  JobProcessingTweets:
    Type: AWS::Glue::Job
    Properties:
      MaxRetries: 0
      GlueVersion: 2.0
      NumberOfWorkers: 2
      Timeout: 20
      WorkerType: "Standard"
      Command:
        Name: glueetl
        ScriptLocation: !Ref PathGlueJobScript  #"s3://tweets-meta-data/scripts/get_entities_sentiment.py"
      DefaultArguments:
        "--job-bookmark-option": "job-bookmark-enable"
        '--TempDir': !Ref GlueJobTempPath
        "--DataBase1": !FindInMap [DatabaseNameMap, !Ref Environmentval, database]
        "--bucket": !Ref DataBucket
        "--env": !Ref Environmentval
      Name: !Join ["-", [!Ref Environmentval , "JobProcessingTweets"]] 
      Role: !Ref RoleProcessingTweets

 #Create a crawler to crawl the processed tweets tables in a s3 bucket
  CrawlerProcessedTweetsTables:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Join ["-", [!Ref Environmentval , "CrawlerProcessedTweetsTables"]]  
      Role: !GetAtt RoleProcessingTweets.Arn
      # Just Crawl New Order to limit workload and costs
      RecrawlPolicy: 
        RecrawlBehavior: "CRAWL_NEW_FOLDERS_ONLY"
      #Classifiers: none, use the default classifier
      Description: AWS Glue crawler to crawl processed tweets tables
      #Schedule: none, use default run-on-demand
      DatabaseName: !FindInMap [DatabaseNameMap, !Ref Environmentval, database]
      TablePrefix: !Join ["_", [!Ref Environmentval , ""]]
      Targets:
        S3Targets:
          # Private S3 bucket with ingested tweet data
          - Path: !Ref PathProcessedTweets 
      SchemaChangePolicy:
        UpdateBehavior: "LOG" # Updated to LOG since RecrawPolicy required it --> "UPDATE_IN_DATABASE"
        DeleteBehavior: "LOG"
      #Configuration: "{\"Version\":1.0,\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"},\"Tables\":{\"AddOrUpdateBehavior\":\"MergeNewColumns\"}},\"Grouping\":{\"TableLevelConfiguration\":4}}"
      Configuration: "{\"Version\":1.0,\"Grouping\":{\"TableGroupingPolicy\":\"CombineCompatibleSchemas\"},\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"},\"Tables\":{\"AddOrUpdateBehavior\":\"MergeNewColumns\"}}}"
  TriggerCrawlerForIngestedTweets:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ["-", [!Ref Environmentval , "TriggerCrawlerForIngestedTweets"]] 
      Type: SCHEDULED
      Schedule: "cron(15 0/1 * * ? *)"
      StartOnCreation: true
      Description: This Trigger triggers the glue crawler for the ingested tweets
      WorkflowName: !Ref ProcessTweetsWorkflow
      Actions:
        - CrawlerName: !Ref CrawlerIngestedTweets

  TriggerJobForProcessingTweets:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ["-", [!Ref Environmentval , "TriggerJobForProcessingTweets"]] 
      Type: CONDITIONAL
      StartOnCreation: True
      Description: After the Crawler CrawlerIngestedTweets is completed the Glue Job
      Actions:
        - JobName: !Ref JobProcessingTweets
          Arguments:
            "--job-bookmark-option": "job-bookmark-enable"
      Predicate:
        Conditions:
          - LogicalOperator: EQUALS
            CrawlerName: !Ref CrawlerIngestedTweets
            CrawlState: SUCCEEDED
      WorkflowName: !Ref ProcessTweetsWorkflow


  TriggerCrawlerForProcessedTweets:
    Type: AWS::Glue::Trigger
    Properties:
      Name: !Join ["-", [!Ref Environmentval , "TriggerCrawlerForProcessedTweets"]]  
      Type: CONDITIONAL
      StartOnCreation: True
      Description: After the Glue Job processed the raw Tweets and processed data in a s3 bucket a Crawler is triggered to crawl the process data and to ceate a Athena table for visualization purposes.
      Actions:
        - CrawlerName: !Ref CrawlerProcessedTweetsTables
      Predicate:
        Conditions:
          - LogicalOperator: EQUALS
            JobName: !Ref JobProcessingTweets
            State: SUCCEEDED
      WorkflowName: !Ref ProcessTweetsWorkflow
